# 第五门课 序列模型(Sequence Models)

## 第一周 循环序列模型（Recurrent Neural Networks）

### 循环神经网络模型（Recurrent Neural Network Model）

使用一般神经网络处理序列问题：

![img](http://www.ai-start.com/dl2017/images/1653ec3b8eb718ca817d3423ae3ca643.png)

一、是输入和输出数据在不同例子中可以有不同的长度，不是所有的例子都有着同样输入长度或是同样输出长度的。即使每个句子都有最大长度，也许你能够填充（**pad**）或零填充（**zero pad**）使每个输入语句都达到最大长度，但仍然看起来不是一个好的表达方式。

二、一个像这样单纯的神经网络结构，它并不共享从文本的不同位置上学到的特征。具体来说，如果神经网络已经学习到了在位置1出现的**Harry**可能是人名的一部分，那么如果**Harry**出现在其他位置，比如时，它也能够自动识别其为人名的一部分的话，这就很棒了。这可能类似于你在卷积神经网络中看到的，你希望将部分图片里学到的内容快速推广到图片的其他部分，而我们希望对序列数据也有相似的效果。和你在卷积网络中学到的类似，用一个更好的表达方式也能够让你减少模型中参数的数量。

**RNN**前向传播示意图：

![nn-](http://www.ai-start.com/dl2017/images/rnn-f.png)

![img](http://www.ai-start.com/dl2017/images/140529e4d7531babb5ba21778cd88bc3.png)

把复杂的示意图拆分成多个模块组合，每个数据的激活值计算还需要其前一层的激活值。

即RNN在某一时刻的预测仅使用了从序列之前的输入信息。

前向传播的计算过程：

![img](http://www.ai-start.com/dl2017/images/19cbb2d356a2a6e0f35aa2a946b23a2a.png)



在t时刻：

![image-20200710171302237](/Users/mac/Library/Application Support/typora-user-images/image-20200710171302237.png)

在此基础上简化：
![img](http://www.ai-start.com/dl2017/images/27afdd27f45ad8ddf78677af2a3eeaf8.png)

即把权重矩阵Waa和Wax水平压缩为一个矩阵Wa，对应的a和x也垂直何必为一个向量。

### 通过时间的反向传播（Backpropagation through time）

![img](http://www.ai-start.com/dl2017/images/71a0ed918704f6d35091d8b6d60793e4.png)

由前向传播得到时间t的损失为：

![image-20200710172523164](/Users/mac/Library/Application Support/typora-user-images/image-20200710172523164.png)

此处使用标准逻辑回归损失函数，也叫交叉熵损失函数（**Cross Entropy Loss**）,是关于单个位置上或者说某个时间步t上某个单词的预测值的损失函数。

因此计算出每个时刻的损失，将其加起来，得到RNN的损失函数：

![image-20200710172605876](/Users/mac/Library/Application Support/typora-user-images/image-20200710172605876.png)

RNN的反向传播如图红色线表示，与前向传播恰好相反，从右到左逐步计算。

![nn_cell_backpro](http://www.ai-start.com/dl2017/images/rnn_cell_backprop.png)

### 不同类型的循环神经网络（Different types of **RNN**s）

![img](http://www.ai-start.com/dl2017/images/1daa38085604dd04e91ebc5e609d1179.png)

### 语言模型和序列生成（Language model and sequence generation）

使用**RNN**构建一个语言模型

语言模型所做的就是，它会估计某个句子序列中各个单词出现的可能性，告诉你某个特定的句子它出现的概率是多少。



首先需要一个训练集，包含一个很大的英文文本语料库（**corpus**）或者其它的语言，你想用于构建模型的语言的语料库。语料库是自然语言处理的一个专有名词，意思就是很长的或者说数量众多的英文句子组成的文本。



然后将输入的句子标记化，建立一个字典，然后将每个单词都转换成对应的**one-hot**向量，也就是字典中的索引。可能还有一件事就是你要定义句子的结尾，一般的做法就是增加一个额外的标记，叫做**EOS**，它表示句子的结尾，这样能够帮助你搞清楚一个句子什么时候结束。

![img](http://www.ai-start.com/dl2017/images/54dfcac1220e3cf567a2c656383e40ec.png)





构建一个**RNN**来构建这些序列的概率模型。

![img](http://www.ai-start.com/dl2017/images/986226c39270a1e14643e8658fe6c374.png)





### 对新序列采样（Sampling novel sequences）

训练一个序列模型之后，要想了解到这个模型学到了什么，一种非正式的方法就是进行一次新序列采样。

### 循环神经网络的梯度消失（Vanishing gradients with **RNN**s）

训练很深的神经网络时，随着层数的增加，导数有可能指数型的下降或者指数型的增加，我们可能会遇到梯度消失或者梯度爆炸的问题。

加入一个**RNN**处理1,000个时间序列的数据集或者10,000个时间序列的数据集，这就是一个1,000层或者10,000层的神经网络，这样的网络就会遇到上述类型的问题。

梯度爆炸基本上用梯度修剪就可以应对，但梯度消失比较棘手。

以两个句子为例：

**The cat, which already ate ……, was full.**

**The cats, which ate ……, were full.**

前后应该保持一致，因为**cat**是单数，所以应该用**was**。**cats**是复数，所以用**were**。

句子有长期的依赖，最前面的单词对句子后面的单词有影响,但RNN不擅长于扑捉这种依赖，某个时间步的输出只与附近的输入有关，因此**RNN**不擅长处理长期依赖的问题

### **GRU**单元（Gated Recurrent Unit（**GRU**））

使用**GRU**，门控循环单元网络，可以有效地解决梯度消失的问题，并且能够使你的神经网络捕获更长的长期依赖，它改变了**RNN**的隐藏层，使其可以更好地捕捉深层连接，并改善了梯度消失问题。

简化过的**GRU**单元

![img](http://www.ai-start.com/dl2017/images/cfa628f62f1c57ee6213793a438957a3.png)



### 长短期记忆（**LSTM**（long short term memory）unit）

![image-20200711140636398](/Users/mac/Library/Application Support/typora-user-images/image-20200711140636398.png)

![img](http://www.ai-start.com/dl2017/images/94e871edbd87337937ce374e71d56e42.png)

**LSTM**前向传播图：

![ST](http://www.ai-start.com/dl2017/images/LSTM.png)

![STM_rn](http://www.ai-start.com/dl2017/images/LSTM_rnn.png)

 **LSTM**反向传播计算：

**门求偏导：**

![image-20200711141037692](/Users/mac/Library/Application Support/typora-user-images/image-20200711141037692.png)

**参数求偏导 ：**

![image-20200711141112236](/Users/mac/Library/Application Support/typora-user-images/image-20200711141112236.png)



![image-20200711141145346](/Users/mac/Library/Application Support/typora-user-images/image-20200711141145346.png)

最后，计算隐藏状态、记忆状态和输入的偏导数：

![image-20200711141206450](/Users/mac/Library/Application Support/typora-user-images/image-20200711141206450.png)







### 双向循环神经网络（Bidirectional **RNN**）

在序列的某点处不仅可以获取之前的信息，还可以获取未来的信息.

LSTM和GRU可理解为RNN中每一个时间步所使用的单元结构，BRNN可使用RNN/GRU/LSTM。

![img](http://www.ai-start.com/dl2017/images/48c787912f7f8daee638dd311583d6cf.png)

 

### 深层循环神经网络（Deep **RNN**s）

![img](http://www.ai-start.com/dl2017/images/8378c2bfe73e1ac9f85d6aa79b71b5eb.png)





































