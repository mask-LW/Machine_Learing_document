# 第一门课 神经网络和深度学习

## 第一周：深度学习引言(Introduction to Deep Learning)

### 课程概述

在第一门课程中，你将学习如何建立神经网络（包含一个深度神经网络），以及如何在数据上面训练他们。在这门课程的结尾，你将用一个深度神经网络进行辨认猫。

在第二门课中，我们将使用三周时间。你将进行深度学习方面的实践，学习严密地构建神经网络，如何真正让它表现良好，因此你将要学习超参数调整、正则化、诊断偏差和方差以及一些高级优化算法，比如**Momentum**和**Adam**算法，犹如黑魔法一样根据你建立网络的方式。

在第三门课中，我们将使用两周时间来学习如何结构化你的机器学习工程。事实证明，构建机器学习系统的策略改变了深度学习的错误。

在第四门课程中，我们将会提到卷积神经网络(**CNN(s)**)，它经常被用于图像领域，你将会在第四门课程中学到如何搭建这样的模型。

最后在第五门课中，你将会学习到序列模型，以及如何将它们应用于自然语言处理，以及其它问题。序列模型包括的模型有循环神经网络（**RNN**）、全称是长短期记忆网络（**LSTM**）。你将在课程五中了解其中的时期是什么含义，并且有能力应用到自然语言处理（**NLP**）问题。

### 神经网络

神经网络方面的一个巨大突破是从**sigmoid**函数转换到一个**ReLU**函数

使用**sigmoid**函数和机器学习使sigmoid函数的梯度会接近零，所以学习的速度会变得非常缓慢，因为当你实现梯度下降以及梯度接近零的时候，参数会更新的很慢，所以学习的速率也会变的很慢。

通过改变激活函数，即换用这一个函数，叫做**ReLU**的函数（修正线性单元**Rectified Linear Unit**）。

**ReLU**它的梯度对于所有输入的负值都是零，因此梯度更加不会趋向逐渐减少到零。而这里的梯度，这条线的斜率在这左边是零，仅仅通过将**Sigmod**函数转换成**ReLU**函数，便能够使得一个叫做梯度下降（**gradient descent**）的算法运行的更快。

![img](http://www.ai-start.com/dl2017/images/1a3d288dc243ca9c5a70a69799180c4a.png)

### 深度学习兴起

![img](http://www.ai-start.com/dl2017/images/7a9a642c6c709d19eb0dd2b241f1ebaf.png)

深度学习的性能随着数量的提升而不断提升直至接近稳定，数字化社会的来临使得我们可以获得大量数据远超过机器学习算法能够高效发挥它们优势的规模，导致深度学习的快速发展。

如今最可靠的方法来在神经网络上获得更好的性能，往往就是**要么训练一个更大的神经网络，要么投入更多的数据**，但这只能在一定程度上起作用。

## 第二周：神经网络的编程基础(Basics of Neural Network programming)

### 使用逻辑回归解决二分类问题

<!--逻辑回归可以看做是一个非常小的神经网络-->

假如你有一张图片作为输入，比如这只猫，如果识别这张图片为猫，则输出标签1作为结果；如果识别出不是猫，那么输出标签0作为结果。现在我们可以用字母y来表示输出的结果标签,即 y=0 or1？

图片在计算机的表示：

![img](http://www.ai-start.com/dl2017/images/1e664a86fa2014d5212bcb88f1c419cf.png) 

保存一张图片，需要保存三个矩阵，它们分别对应图片中的红、绿、蓝三种颜色通道，如果你的图片大小为64x64像素，那么你就有三个规模为64x64的矩阵，分别对应图片中红、绿、蓝三种像素的强度值。图中的规模为5x4 而不是64x64。

定义一个特征向量x来表示这张图片：

把所有的像素都取出来，例如255、231等等，直到取完所有的红色像素，接着最后是255、134、…、255、134等等，直到得到一个特征向量，把图片中所有的红、绿、蓝像素值都列出来。

如果图片的大小为64x64像素，那么向量 的总维度，将是64乘以64乘以3，这是三个像素矩阵中像素的总量。在这个例子中结果为12,288。现在我们用，12,288来表示输入特征向量的维度。

![image-20200610164837389](/Users/mac/Library/Application Support/typora-user-images/image-20200610164837389.png)

此时的问题为：

如果X1（一个nx维的向量）是我们在上个视频看到的图片，你想让y^来告诉你这是一只猫的图片的机率有多大。

用w、b来表示逻辑回归的参数，则该问题可以尝试却不可行的函数为![image-20200610165140331](/Users/mac/Library/Application Support/typora-user-images/image-20200610165140331.png)

但由该函数得到的值可能远大于1，也可能是负值，因此逻辑回归中把这个函数所得值z作为sigmoid函数的自变量。

sigmoid函数：![image-20200610165451472](/Users/mac/Library/Application Support/typora-user-images/image-20200610165451472.png)

其图像为：

![img](http://www.ai-start.com/dl2017/images/7e304debcca5945a3443d56bcbdd2964.png)

此时保证了预测得到的值只能在0～1之间。

参数可理解为：

在某些例子里，你定义一个额外的特征称之为x0，并且使它等于1，那么现在X1就是一个nx加1维的变量，然后定义假设函数为：![image-20200610165651905](/Users/mac/Library/Application Support/typora-user-images/image-20200610165651905.png)

参数向量包含：𝞱0、𝞱1...𝞱n，此时的𝞱0充当了b，剩下的𝞱1...𝞱n即充当了w，注意参数维度为(nx+1)x1。

完整的逻辑回归的假设函数：

![1](http://www.ai-start.com/dl2017/images/4c9a27b071ce9162dbbcdad3393061d2.png)

为了训练逻辑回归模型的参数b和参数w，我们需要一个代价函数J，通过训练代价函数来得到参数。

用y^表示模型的预测值，希望它会接近于训练集中的y值。

损失函数又叫做误差函数，用来衡量算法的运行情况，Loss function: ![image-20200610170402933](/Users/mac/Library/Application Support/typora-user-images/image-20200610170402933.png)

在逻辑回归中用到的损失函数是：![image-20200610170219197](/Users/mac/Library/Application Support/typora-user-images/image-20200610170219197.png)

损失函数是在单个训练样本中定义的，它衡量的是算法在单个训练样本中表现如何。

为了衡量算法在全部训练样本上的表现如何，需要定义一个算法的代价函数![image-20200610170546311](/Users/mac/Library/Application Support/typora-user-images/image-20200610170546311.png)

训练逻辑回归模型时候，我们需要找到合适的w和b，来让代价函数J的总代价降到最低。 

通过最小化代价函数来得到参数：

![image-20200610171229899](/Users/mac/Library/Application Support/typora-user-images/image-20200610171229899.png)

𝝰表示学习率（**learning rate**），用来控制步长（**step**）。

### 向量化实现算法

非向量实现逻辑：如果有多个特征，还需要遍历特征数来获得dwi

![image-20200613160718807](/Users/mac/Library/Application Support/typora-user-images/image-20200613160718807.png)

向量实现逻辑回归

![image-20200610174307922](/Users/mac/Library/Application Support/typora-user-images/image-20200610174307922.png)

### Python 中的广播（Broadcasting in Python）

**numpy**广播机制：如果两个数组的后缘维度的轴长度相符或其中一方的轴长度为1，则认为它们是广播兼容的。广播会在缺失维度和轴长度为1的维度上进行。



1⃣️矩阵 Amxn和矩阵B1xn 进行四则运算，后缘维度轴长度相符，可以广播，广播沿着轴长度为1的轴进行，B的每行会复制m遍使其成Bmxn，之后做逐元素四则运算。

2⃣️矩阵 Amxn和矩阵Bnx1 进行四则运算，后缘维度轴长度不相符，可以广播，广播沿着轴长度为1的轴进行，B的每列会复制n遍使其成Bmxn，之后做逐元素四则运算。

3⃣️矩阵Amx1和常数进行四则运算，后缘维度轴长度不相符，可以广播，广播沿着轴长度为1的轴进行，常数每行复制m遍使其成Bmx1，之后做逐元素四则运算。

4⃣️矩阵A1xn和常数进行四则运算，后缘维度轴长度不相符，可以广播，广播沿着轴长度为1的轴进行，常数每列复制n遍使其成B1xn，之后做逐元素四则运算。

## 第三周：浅层神经网络(Shallow neural networks)

