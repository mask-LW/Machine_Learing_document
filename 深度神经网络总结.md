深度神经网络总结



1⃣️CNN卷积神经网络



卷积神经网络（ConvolutionalNeuralNetwork，CNN）最初是为解决图像识别等问题设计的，CNN现在的应用已经不限于图像和视频，也可用于时间序列信号，比如音频信号和文本数据等。

与一般的DNN模型相比，CNN模型独有两个隐藏层，分别为卷积层（Convolution Layer）和池化层(Pooling layer)，卷积层的激活函数使用的是ReLU，池化层没有激活函数。

一个典型的卷积神经网络通常有三层，一个是卷积层，我们常常用**Conv**来标注。

还有两种常见类型的层，一个是池化层，我们称之为**POOL**。

最后一个是全连接层，用**FC**表示。

虽然仅用卷积层也有可能构建出很好的神经网络，但大部分神经网络架构师依然会添加池化层和全连接层。

幸运的是，池化层和全连接层比卷积层更容易设计。

因此，想要高度CNN，只需要在搞懂DNN的基础上，把卷积层和池化层的原理搞清楚。



卷积运算是卷积神经网络最基本的组成部分



**Padding**

如果我们有一个n x n的图像，用的f x f过滤器做卷积，那么输出的维度就是（n-f+1）x (n-f+1)。

如此进行的卷积操作就会出现两个问题：

1.每次卷积后输出变小

2.那些在角落或者边缘区域的像素点在输出中采用较少，意味着你丢掉了图像边缘位置的许多信息。



解决方法：在卷积操作前填充图像。【Padding】

比如6x6的图像卷积操作前填充为8x8的图像，再用3x3的过滤器进行卷积后得到就是6x6的图像。

当填充数量为p，则输出变为（n-f+2p+1）x （n-f+2p+1).



填充方法：Valid卷积和Same卷积

Valid卷积即不填充像素，即p=0

Same卷积即以上方法，求解n-f+2p+1 = n，当f是一个奇数，p = （f-1）/2

一般f都为奇数。



**卷积步长**（Strided convolution）

使用3x3的过滤器进行卷积7x7的图像时，设置卷积步长s=2，则得到一个3x3的输出。

即输出变为 (（n-f+2p）/s + 1 ) x  (（n-f+2p）/s + 1 ) 

当（n-f+2p）/s不是整数时向下取整



![image-20200601183934103](/Users/mac/Library/Application Support/typora-user-images/image-20200601183934103.png)

**三维卷积**（Convolutions over volumes）



![image-20200601200602708](/Users/mac/Library/Application Support/typora-user-images/image-20200601200602708.png)





如果你有一个 n x n x nc（通道数）的输入图像，在这个例子中就是6×6×3，这里的nc就是通道数目，然后卷积上一个f x f x nc，这个例子中是3×3×3，按照惯例，这个（前一个nc）和这个（后一个nc）必须数值相同。然后你就得到了（n-f+1）x （n-f+1）x nc'，这里的nc‘其实就是下一层的通道数，它就是你用的过滤器的个数，在我们的例子中，那就是4×4×2。我写下这个假设时，用的步幅为1，并且没有padding。如果你用了不同的步幅或者padding，那么这个数值会变化，正如前面的视频演示的那样。

**单层卷积网络**（one layer of a convolution network）

--建立CNN的卷积层，了解卷积层的工作原理以及如何计算输出映射到下一层作为激活值

⚠️卷积层应用非线性函数ReLu

以上面为例，从6×6×3的输入推导出一个4×4×2矩阵，它是卷积神经网络的一层，把它映射到标准神经网络中四个卷积层中的某一层或者一个非卷积神经网络中。

在前向传播中，输入和相关过滤器等于执行线性函数，再加上偏差，作为该卷积层的输入z，然后应用于非线性函数ReLu得到4×4×2矩阵

具体：

首先执行线性函数，然后所有元素相乘做卷积，具体做法是运用线性函数再加上偏差，然后应用激活函数**ReLU**。这样就通过神经网络的一层把一个6×6×3的维度a0演化为一个4×4×2维度的a1，这就是卷积神经网络的一层。

🌟与一般全连接层对比，不考虑数据形式，实质仍是输入与参数相乘后加上偏差，最后执行sigmoid函数，但这里是ReLu。

![img](http://www.ai-start.com/dl2017/images/53d04d8ee616c7468e5b92da95c0e22b.png)



**池化层**(Pooling layers)

--使用池化层缩减模型大小，提高计算速度，提高所提取特征的鲁棒性【健壮性】



![img](http://www.ai-start.com/dl2017/images/ad5cf6dd7ca9a8ef144d8d918b21b1bc.png)

假如输入是一个4×4矩阵，用到的池化类型是最大池化（**max pooling**）。执行最大池化的树池是一个2×2矩阵。执行过程非常简单，把4×4的输入拆分成不同的区域，我把这个区域用不同颜色来标记。对于2×2的输出，输出的每个元素都是其对应颜色区域中的最大元素值。

计算卷积层输出大小的公式同样适用于最大池化，即(（n-f+2p）/s + 1 ) ，这个公式也可以计算最大池化的输出大小。

🌟还有一种池化为平均池化，选取的不是某个区域的最大值，而是该区域的平均值

最大池化只是计算神经网络某一层的静态属性。

计算神经网络有多少层时，通常只统计具有权重和参数的层。因为池化层没有权重和参数，只有一些超参数。这里，我们把**CONV1**和**POOL1**共同作为一个卷积，并标记为**Layer1**。



至于如何选定这些参数，后面我提供更多建议。常规做法是，尽量不要自己设置超参数，而是查看文献中别人采用了哪些超参数，选一个在别人任务中效果很好的架构，那么它也有可能适用于你自己的应用程序



**卷积神经网络的优势**

--卷积神经网络在计算机视觉任务中表现良好

参数共享：输入矩阵共享过滤器

稀疏连接：每次卷积运算只需要用到输入矩阵中同过滤器大小相同的元素。

